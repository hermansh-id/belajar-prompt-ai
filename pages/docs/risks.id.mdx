
# Risiko & Penyalahgunaan

import { Callout } from 'nextra-theme-docs'
import {FilesIcon} from 'components/icons'
import ContentFileNames from 'components/ContentFileNames'

Prompt (perintah atau pertanyaan) yang dirancang dengan baik dapat menghasilkan penggunaan LLM (Model Bahasa Besar) yang efektif untuk berbagai tugas. Ini dapat dilakukan dengan menggunakan teknik seperti pembelajaran contoh terbatas (few-shot learning) dan prompting berantai (chain-of-thought prompting). 

Saat Anda mulai memikirkan untuk membangun aplikasi nyata menggunakan LLM, penting juga untuk mempertimbangkan potensi penyalahgunaan, risiko, dan praktik keamanan yang terkait dengan model bahasa ini.

Bagian ini berfokus pada beberapa risiko dan penyalahgunaan LLM, misalnya melalui teknik seperti injeksi prompt (prompt injection). Injeksi prompt adalah situasi di mana seseorang mencoba memanipulasi output LLM dengan memasukkan perintah yang tidak diinginkan ke dalam input. 

Kami juga akan membahas perilaku berbahaya yang mungkin muncul dan cara mengatasinya. Beberapa solusi yang akan kita bahas termasuk teknik prompting yang efektif dan penggunaan alat seperti API moderasi (moderation APIs). API moderasi adalah layanan yang dapat membantu menyaring konten berbahaya atau tidak pantas.

Topik penting lainnya yang akan kita bahas meliputi:
- Generalisasi: Kemampuan LLM untuk menerapkan pengetahuan pada situasi baru
- Kalibrasi: Memastikan output LLM sesuai dengan tingkat kepercayaan yang tepat
- Bias: Kecenderungan LLM untuk menghasilkan output yang tidak adil atau berat sebelah
- Bias sosial: Bagaimana LLM dapat memperkuat stereotip atau prasangka sosial
- Faktualitas: Sejauh mana informasi yang dihasilkan LLM dapat diandalkan dan akurat

Dengan memahami risiko dan tantangan ini, kita dapat menggunakan LLM dengan lebih bertanggung jawab dan efektif.

<ContentFileNames section="risks" lang="en"/>

