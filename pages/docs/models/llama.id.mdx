
## LLaMA: Model Bahasa Dasar yang Terbuka dan Efisien

<Callout emoji="⚠️">
  Bagian ini sedang dalam pengembangan intensif.
</Callout>


import {Screenshot} from 'components/screenshot'
import { Callout, FileTree } from 'nextra-theme-docs'
import LLAMA1 from '../../../img/llama-1.png'


## Apa yang baru?

Makalah ini memperkenalkan serangkaian model bahasa dasar dengan parameter mulai dari 7 miliar hingga 65 miliar. 

Model-model ini dilatih menggunakan triliunan token dari dataset yang tersedia untuk umum.

Penelitian oleh [(Hoffman et al. 2022)](https://arxiv.org/abs/2203.15556) menunjukkan bahwa dengan anggaran komputasi yang sama, model yang lebih kecil yang dilatih dengan data jauh lebih banyak dapat mencapai kinerja yang lebih baik daripada model yang lebih besar. Penelitian tersebut merekomendasikan pelatihan model 10 miliar parameter dengan 200 miliar token. Namun, makalah LLaMA menemukan bahwa kinerja model 7 miliar parameter terus meningkat bahkan setelah dilatih dengan 1 triliun token.

<Screenshot src={LLAMA1} alt="LLAMA1" />

Penelitian ini berfokus pada pelatihan model (LLaMA) yang mencapai kinerja terbaik pada berbagai anggaran inferensi, dengan cara melatihnya menggunakan lebih banyak token.


## Kemampuan & Hasil Utama

Secara keseluruhan, LLaMA-13B mengungguli GPT-3 (175B) dalam banyak pengujian meskipun ukurannya 10 kali lebih kecil dan dapat dijalankan pada satu GPU. LLaMA 65B mampu bersaing dengan model-model seperti Chinchilla-70B dan PaLM-540B.


*Makalah:* [LLaMA: Model Bahasa Dasar yang Terbuka dan Efisien](https://arxiv.org/abs/2302.13971)

*Kode:* https://github.com/facebookresearch/llama

## Referensi

- [Koala: Model Dialog untuk Penelitian Akademis](https://bair.berkeley.edu/blog/2023/04/03/koala/) (April 2023)
- [Baize: Model Obrolan Sumber Terbuka dengan Penyetelan Parameter Efisien pada Data Obrolan Mandiri](https://arxiv.org/abs/2304.01196) (April 2023)
- [Vicuna: Chatbot Sumber Terbuka yang Mengesankan GPT-4 dengan Kualitas 90%* ChatGPT](https://vicuna.lmsys.org/) (Maret 2023)
- [LLaMA-Adapter: Penyetelan Efisien Model Bahasa dengan Perhatian Zero-init](https://arxiv.org/abs/2303.16199) (Maret 2023)
- [GPT4All](https://github.com/nomic-ai/gpt4all) (Maret 2023)
- [ChatDoctor: Model Obrolan Medis yang Disetel Halus pada Model LLaMA Menggunakan Pengetahuan Domain Medis](https://arxiv.org/abs/2303.14070) (Maret 2023)
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) (Maret 2023)
