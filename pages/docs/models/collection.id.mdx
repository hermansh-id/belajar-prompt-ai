

# Koleksi Model Bahasa Besar (LLM)

import { Callout, FileTree } from 'nextra-theme-docs'

Bagian ini berisi kumpulan dan ringkasan Model Bahasa Besar (LLM) yang penting dan mendasar.

## Model-Model

| Model | Tanggal Rilis | Ukuran (Miliar Parameter) | Tautan Unduhan | Deskripsi |
| --- | --- | --- | --- | --- |
| [Falcon LLM](https://falconllm.tii.ae/) | Sep 2023 | 7, 40, 180 | [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b), [Falcon-180B](https://huggingface.co/tiiuae/falcon-180B) | Falcon LLM adalah model bahasa besar (LLM) dasar dengan 180 miliar parameter yang dilatih menggunakan 3500 miliar token. TII telah merilis Falcon LLM – model 180 miliar parameter. |
| [Mistral-7B-v0.1](https://arxiv.org/abs/2310.06825) | Sep 2023 | 7 | [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | Mistral-7B-v0.1 adalah model teks generatif yang telah dilatih sebelumnya dengan 7 miliar parameter. Model ini menggunakan arsitektur transformer dengan fitur-fitur seperti Grouped-Query Attention, tokenizer Byte-fallback BPE, dan Sliding-Window Attention. |
| [CodeLlama](https://scontent.fbze2-1.fna.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=aLQJyBvzDUwAX-5EVhT&_nc_ht=scontent.fbze2-1.fna&oh=00_AfA2dCIqykviwlY3NiHIFzO85n1-JyK4_pM24FJ5v5XUOA&oe=6535DD4F) | Agu 2023 |7, 13, 34 | [CodeLlama-7B](https://huggingface.co/codellama/CodeLlama-7b-hf), [CodeLlama-13B](https://huggingface.co/codellama/CodeLlama-13b-hf), [CodeLlama-34B](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf) | Keluarga Code Llama dirancang untuk sintesis dan pemahaman kode secara umum. Model ini dioptimalkan untuk mengikuti instruksi dan penerapan yang lebih aman. Model-model ini bersifat auto-regresif dan menggunakan arsitektur transformer yang dioptimalkan. Model-model ini ditujukan untuk penggunaan komersial dan penelitian dalam bahasa Inggris dan bahasa pemrograman yang relevan. |
| [Llama-2](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) | Jul 2023 | 7, 13, 70 | [Llama-2-7B](https://huggingface.co/meta-llama/Llama-2-7b), [Llama-2-13B](https://huggingface.co/meta-llama/Llama-2-13b), [Llama-2-70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) | LLaMA-2, dikembangkan oleh Meta AI, dirilis pada Juli 2023 dengan model berukuran 7, 13, dan 70 miliar parameter. Model ini mempertahankan arsitektur yang mirip dengan LLaMA-1 tetapi menggunakan 40% lebih banyak data pelatihan. LLaMA-2 mencakup model dasar dan model yang disetel untuk dialog, yang dikenal sebagai LLaMA-2 Chat, dan tersedia untuk banyak penggunaan komersial, dengan beberapa batasan. |

(Catatan: Daftar model di atas hanya sebagian kecil dari daftar lengkap yang ada dalam konten asli. Saya telah mempersingkatnya untuk keperluan demonstrasi.)

<Callout emoji="⚠️">
  Bagian ini masih dalam tahap pengembangan.
</Callout>

Data diadopsi dari [Papers with Code](https://paperswithcode.com/methods/category/language-models) dan penelitian terbaru oleh [Zhao et al. (2023)](https://arxiv.org/pdf/2303.18223.pdf).

